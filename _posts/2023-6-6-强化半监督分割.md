---
layout: post
title: 强化半监督分割
tags: 论文阅读
math: true
toc: true
date: 2023-6-6 10:56 +0800
---

人生的大起大落吧，在我最焦虑的时候。突然收到科大和上交的一面通知，而且都还顺利通过了。现在在准备科大和上交的二面，虽然很忙碌，压力很大，但是反而却没有那么焦虑了。这篇blog就算是，科大二面的草稿吧。

# 0，PPT大概思路

preliminary：神经元分割是什么，EM数据的方法变革什么的

Motivation：数据较少，标注贵什么的

Related Work：半监督方法，一致性学习什么的

Methodology：他里面提出的方法，加了一步

Result：他的结果消融实验什么的

Demo：测试一下

行文的思路就是：

1，神经元分割是神经元重建中的重要一步

2，实现自动神经元分割需要大量的手工注释，成本高

3，无足够手工注释，模型会变得脆弱（不鲁棒），因此使用被称为一致性学习的半监督策略

4，一致性学习只适用于对未标记数据的有意义预测，因此设置了一个代理任务重构卷

# 1，Preliminary

![id5884](D:\pypro\xiejingcheng.github.io\xiejingcheng.github.io\_posts\img\id5884.png)

![img](D:\pypro\xiejingcheng.github.io\xiejingcheng.github.io\_posts\img\u=347690991,646356327&fm=253&fmt=auto&app=138&f=PNG.png)

![img](D:\pypro\xiejingcheng.github.io\xiejingcheng.github.io\_posts\img\1414369-20181114152744186-1078345422.png)

![img](D:\pypro\xiejingcheng.github.io\xiejingcheng.github.io\_posts\img\84731af9fb9b3a08c2812e3a9b4353f8.jpg)

自动神经元分割的方法在过去的几十年中进行了不断的迭代和改进。这些方法旨在通过计算机算法和图像处理技术，从大量的神经影像数据中准确地提取和分割出神经元结构。初始的自动分割方法主要依赖于简单的阈值分割或者基于形状和强度特征的手工设计的规则。然而，这些方法在处理复杂的神经元形状和变化的图像条件时往往表现不佳。

随着深度学习技术的兴起，神经网络在神经元分割中得到了广泛应用。初始的深度学习方法主要采用全卷积网络（FCN）来进行像素级别的分割，但是由于神经元结构的复杂性和数据缺乏标注，单纯的监督学习方法往往面临着数据不足和标注成本高的问题。

为了解决这些问题，研究人员开始探索半监督学习和弱监督学习方法。半监督学习利用有标签和无标签的数据进行训练，通过利用未标注数据的信息来提高模型的性能。弱监督学习则利用部分标注的数据进行训练，从而降低了标注的成本。

此外，近年来，研究人员还提出了基于生成对抗网络（GAN）和变分自编码器（VAE）的方法，用于进行神经元分割和重建。这些方法通过学习数据的分布和生成样本来改善分割的准确性和鲁棒性。

总的来说，自动神经元分割的方法经历了多个迭代阶段，从简单的阈值分割到基于深度学习的方法，再到半监督学习和生成模型的应用。这些迭代不断推动着神经元分割方法的发展，使得神经科学研究人员能够更准确地理解和分析神经元网络的结构和功能。

1. 传统的基于图像处理和机器学习的方法：早期的神经元分割方法主要依赖于图像处理和机器学习技术。这些方法通常基于手工设计的特征提取器和分类器，如边缘检测算法、阈值分割算法、形态学操作等。
2. 基于图割的方法：图割是一种基于图论的优化算法，被广泛应用于神经元分割。这些方法将神经元分割问题转化为图割问题，通过最小化能量函数来优化分割结果。图割方法在处理复杂的神经元结构和噪声较少的数据集上取得了良好的效果。
3. 基于随机森林和条件随机场的方法：随机森林和条件随机场是常用的机器学习方法，用于解决神经元分割问题。这些方法利用像素级别的特征和上下文信息来进行分割，通过训练分类器和优化能量函数来得到最终的分割结果。
4. 基于深度学习的方法：随着深度学习的兴起，神经元分割领域也开始采用深度学习方法。卷积神经网络（CNN）被广泛用于神经元分割任务，通过端到端的训练从原始图像中学习特征表示和分割决策。一些常用的深度学习模型包括U-Net、3D U-Net、Mask R-CNN等。
5. 半监督学习方法：近年来，半监督学习方法在神经元分割中得到了广泛应用。这些方法利用有标签和无标签的数据进行训练，通过一致性学习和自我训练等技术来提高分割性能。这些方法可以更好地利用未标记数据，减少对大量标记数据的需求。

# 2，Related work

在"Related Work"部分，作者通常会回顾和总结与他们的研究问题或方法相关的文献。他们可能会介绍先前的方法、模型或算法，以及这些方法的优点和局限性。此外，他们可能会提到已有工作中的一些关键观点、理论或指标，这些都可以为本文的背景和引用提供支持。

具体而言，"Related Work"部分可能包括以下内容：

1. 先前的研究成果：介绍与本文研究领域相关的关键论文、方法或模型，并概述它们的主要思想、技术或算法。
2. 方法比较和分析：对先前方法进行比较和分析，指出它们的优点和不足之处。可能会讨论一些评估指标、实验设置或方法的优势和局限性。
3. 开放问题和挑战：讨论当前领域中仍然存在的问题、挑战或待解决的难题。这可以为本文的研究提供动机和背景，并强调本文的研究目标和贡献。
4. 相关领域的交叉研究：如果本文的研究涉及多个学科领域或与其他领域有关联，作者可能会介绍与这些领域相关的工作，并探讨这些领域之间的联系和影响。

总之，"Related Work"部分旨在将本文的研究放置在更大的研究背景中，展示作者对相关工作的了解和思考，并凸显本文的创新点和贡献。这部分的内容应该清晰地表达作者对前人工作的批判性思考，并为读者提供一个更全面的视角来理解本文的研究价值。

## Supervised Learning for Neuron Segmentation

# 3，Methodology

 $$D_l = {(x_l^i, y^i)}^N_{i=1} $$ 

 $$D_u = {(x_l^i)}^M_{i=1} $$ 

$$y_i\in \{{0,1}\}^{C\times{D\times{H\times{W}}}} $$


$$
E(\mathbf{L}) = \sum_{i} D_i(\mathbf{L}i) + \sum{i,j} V_{ij}(\mathbf{L}_i, \mathbf{L}_j)
$$

$$
E(x, y) = \begin{cases}
1 & \text{if } \text{Gradient Magnitude}(x, y) \geq \text{High Threshold} \\
0 & \text{if } \text{Gradient Magnitude}(x, y) < \text{Low Threshold} \\
\text{Undecided} & \text{otherwise}
\end{cases}
$$

$$



![image-20230613110607147](D:\pypro\xiejingcheng.github.io\xiejingcheng.github.io\_posts\img\image-20230613110607147.png)



# 999，一些术语

## 冷启动（a cold start stage）

"a cold start stage"指的是一个初始阶段或冷启动阶段。具体来说，它可能是指在开始执行某项任务或算法时，系统或模型需要在没有足够的先验信息或训练数据的情况下进行初始化。

在机器学习和数据分析的上下文中，"a cold start"通常表示在缺乏足够数据或领域知识的情况下启动模型或算法的过程。这可能是因为系统是全新的，没有历史数据可用，或者是由于特定领域的限制，只能获得有限的初始数据。

在这篇文章中，"a cold start stage"可能是指在半监督学习任务中，初始阶段使用了有限的标记数据进行模型训练和初始化。由于标记数据数量有限，需要结合未标记数据进行学习和正则化，以提高模型的性能和泛化能力。这个冷启动阶段可能涉及预训练和一致性学习等技术，以利用未标记数据进行模型改进和优化。

简单来说，就算没有预训练or代理任务直接开始

## 一致性学习

一致性学习（Consistency Learning）是半监督学习中的一种策略或方法，用于利用未标记数据来提升模型的性能和泛化能力。

一致性学习是半监督学习中常用的一种方法。其核心思想是通过对无标签数据进行多次预测，并鼓励这些预测结果之间的一致性来进行训练。具体而言，对于同一无标签样本的多次预测结果，一致性学习通过比较它们之间的差异来定义一致性损失函数，并将其作为正则化项加入到目标函数中。通过最小化一致性损失，模型被鼓励对同一样本进行一致的预测，从而提高模型的泛化能力和鲁棒性。

## 正则化

正则化在半监督学习中起到了重要的作用。正则化是一种用于控制模型复杂度并防止过拟合的技术。在这篇论文中，作者将预训练阶段视为一种正则化过程。他们使用自动编码器进行重建任务，通过重建清洁样本和其扰动副本之间的差异来学习有用的特征表示。这种预训练过程相当于对模型进行正则化，使其在学习阶段更好地捕捉到数据中的结构信息。

## 亲和力预测

什么是亲和关系

## 3D ResUNet

https://zhuanlan.zhihu.com/p/67465359

## 为什么一致性学习需要标签有意义

一致性学习是一种半监督学习方法，旨在通过使用未标记的数据来提升模型的性能。其核心思想是通过对同一样本施加不同的扰动或变换，要求模型对扰动前后的预测结果保持一致。在这种方法中，标签有意义的预测是至关重要的。

标签有意义的预测意味着模型在处理扰动后的样本时，能够产生一致的预测结果。这种一致性可以是指预测的类别标签或分割结果在不同扰动下保持一致，或者是指生成的概率分布在不同扰动下保持一致。通过强调一致性，模型被迫学习到数据的内在结构和一致性特征，而不仅仅是对个别样本的过度拟合。

通过要求标签有意义的预测，一致性学习可以有效提升模型的鲁棒性和泛化能力。当模型能够在扰动后产生一致的预测结果时，它表明模型具有对输入数据的鲁棒性，能够从不同的视角、噪声或变换下识别和理解图像。这种鲁棒性使得模型更适用于真实世界中的复杂场景和未知数据。

此外，标签有意义的预测还可以帮助模型在训练过程中过滤掉随机性和噪声。如果模型在扰动后的预测结果是随机的或不一致的，那么模型很可能没有真正理解样本的特征和语义，而是对噪声或不相关信息产生了过度拟合。通过要求标签有意义的预测，一致性学习可以减少模型对噪声和随机性的敏感性，提高模型的鲁棒性和泛化能力。

因此，一致性学习中要求标签有意义的预测是为了帮助模型从未标记的数据中学习到更多有用的信息，并提升模型的性能、鲁棒性和泛化能力。

## 

"Heuristic decision of thresholds for noisy label rectification" 指的是以启发式（经验法则）方式确定用于修正噪声标签的阈值或标准的过程。在标签修正的背景下，当处理存在噪声或错误标签时，需要确定一个阈值或标准，以识别哪些标签需要进行修正或被视为不可靠。

"启发式决策阈值"表示决策过程是基于实际规则、直觉或领域知识，而不是基于形式化的、数据驱动的方法。它涉及根据观察、假设或以往经验设定某些阈值或条件，以确定何时将标签视为噪声或错误。

启发式决策阈值的目的是建立指导方针或规则，帮助识别和修正错误的标签，而不仅仅依赖于噪声数据本身。这些阈值或标准可以基于统计量、置信度分数、评估者间一致性或其他特定领域的考虑因素。

然而，需要注意的是，启发式决策并不总是最优的，可能对特定数据集或应用程序敏感。因此，必须仔细验证和微调这些阈值，以实现准确的标签修正，提高训练数据的质量。