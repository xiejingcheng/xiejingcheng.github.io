---
layout: post
title: 西瓜书第一章 绪论
tags: ML基础 西瓜书
math: true
mathjax: true
date: 2023-4-14 14:56 +0800
---



开始写这个blog的时候，我似乎处于我保研阶段的第一个低谷，pkusz的老师已读不回，seu的老师说好的面试遥遥无期，nku和xjtu的还没开始套，就是挺难绷的。排名也出来了，奇迹没有发生，但是回头想想7/163的成绩似乎也还不错，虽然sjtu和fdu的CS没有希望了，但是我还是可以试一试类脑的。坚持下来，或许预推免还有希望。

同时学妹要期中考试了，我才发现这个学期已经过去一半了，我似乎在保研套磁上浪费了太多时间，我或许应该花更多的时间提升自己。因此我开始重新学习西瓜书，或许为了的笔试，也可能是为了夏令营的面试，总之还是早做准备吧。也有一部分想法，我只是觉得，打好更加扎实的基础，或许可以帮助我走得更远。

如果有一天这个blog公开了，会被其他人看见，我只是想说，这并不适合初学者来看，而是适合一个像我一样的半吊子来看，把自己零零碎碎的知识拼接成一个完整的体系。

# 一，假设空间

绪论开篇的引言和基本术语似乎没有什么好讲的，之前了解的也八九不离十，所以我压根没怎么看，直接跳到了假设空间。

最开始就说明确了一个基础的概念，**归纳**和**演绎**，其实就说从特殊到一般的泛化，以及从一般到特殊的泛化。

其中最重要的假设空间，在我的理解里面就是：把模型的所有关键点进行参数化，然后把这些参数组成一个个字典，这些字典的集合，就是假设空间。学习的过程，就是在这其中做一个**搜索**，搜索出最合适的参数。这里的参数，有两类：一类是我们常说的超参，就是那个经常要我们人工调的；另一类是模型自己学习的参数，就像深度模型中的权重，XGB的分位点等等。

同时假设的表示一旦确定，假设空间的大小也就确定。就像字典的每个key确定了，value的取值范围也确定了，那这个字典集合的最大上限也确定了。搜索的方法很多，指的可能像是学习策略一类的，就比如SGD和adam之类的。

同时，由于实际问题的假设空间往往很大，而我们的训练集较小，提供的信息不够。所有我们往往可能得到多个假设对应着可以fit训练集，这些假设的集合叫做**版本空间**。至于如何进一步的选择，那就是后话了。

# 二，归纳偏好

归纳偏好，归纳偏置，我对它们第一个深刻的印象就是卷积网络的归纳偏置：平移等变性之类的。我对他的理解就是，算法对于假设选择的一种偏好。学习是一种在假设空间中的搜索，那归纳偏好有点像启发式搜索重点启发函数。一种先验知识。嗯，我的语言表达能力不佳，但是大概可以理解出来。

一个有效的算法应该有一个归纳偏好，就像上文说的，由于训练集相对于实际问题总是很小的，所有会有很多假设在训练集上表现一致，这时候就需要归纳偏置在这些看似等效的假设中做出选择。

文中提到了奥卡姆剃刀原理，大概意思就是，在同样可以使用的情况下，简单的最好。但是实际上，这个假设并不是一直适用，下面就是要引出一个重要的结论：**NFL定理**。

NFL定理就是没有免费午餐定理，指的是：总误差与学习算法无关。这里的总误差指的是，算法对于所有可能的“问题”产生的误差。但是这里有个前提，就是每个问题出现的概率要相同，而且同等重要。这在实际问题中总是不可能的。总结一下就是，如果不分析具体问题，那么每一个算法的性能都是一样的，而每一种算法都是适用于某一种环境。

这里我有个很奇妙的想法，数据预处理的目的之一，是否就是将数据环境处理成一个适合这个算法发挥最大性能的环境呢。

下面可能就是证明NFL定理，其中完全可以看书，但是我还是想练练我敲公式的能力和数学推理的能力吧。

首先我们先定义算法在训练集上的误差为：
$$
E_{ote}(\mathfrak{L}_{a}|X,f)=\sum_{h}\sum_{x\in \chi-X}P(\mathbf{x})\mathbb{I}(h(\mathbf{x})\neq{f(\mathbf{x})})P(h|X,\mathfrak{L}_{a})
$$

其中$P(h|X,\mathfrak{L}_{a})$代表算法$\mathfrak{L}_{a}$基于训练数据$X$产生假设$h$的概率。同时预测$f$为我们我们希望学习的真实目标函数。

如果我们假设所有的可能的$f$都按均匀分布，那么这个算法的总误差为：
$$
\begin{aligned}
\sum_{f}E_{ote}(\mathfrak{L}_{a}|X,f)&=\sum_{f}\sum_{h}\sum_{x\in \chi-X}P(\mathbf{x})\mathbb{I}(h(\mathbf{x})\neq{f(\mathbf{x})})P(h|X,\mathfrak{L}_{a})\\

&=\sum_{x\in \chi-X}P(\mathbf{x})\sum_{h}P(h|X,\mathfrak{L}_{a})\sum_{f}\mathbb{I}(h(\mathbf{x})\neq{f(\mathbf{x})})\\

&=\sum_{x\in \chi-X}P(\mathbf{x})\sum_{h}P(h|X,\mathfrak{L}_{a})\frac{1}{2}2^{\lvert{\chi}\rvert}\\

&=2^{\lvert{\chi}\rvert-1}\sum_{x\in \chi-X}P(\mathbf{x})
\end{aligned}
$$
这里的公式很长，一开始看很复杂实际上理解了很简单，由于$g$,$h$,$x$三者是独立不相关的，所以，第一行到第二行可以改变求和符合的位置。同时，由于每种$f$都是等价的，所以在二分类问题中，就算有一半的会猜测错。最后一步的也很简单：$\sum_{h}P(h|X,\mathfrak{L}_{a})=1$ 条件概率的和为1。

至此我们可以证明出，总误差和算法无关，如果不具体分析问题，那么任意两种算法的性能都一样。



