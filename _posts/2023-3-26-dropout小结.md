---
layout: post
title: dropout的一些思索
tags: ML基础
math: true
date: 2023-3-26 14:56 +0800
---

# 一，前言

dropout rate是我之前经常遇到的一个参数，我之前并未觉得这个参数有多重要，总是随便写个0.3就完事了。但是这次的训练中出现了严重的过拟合现象，这时候我就想起了dropout rate这个参数，通过调整dropout rate似乎对过拟合有出奇好的效果。我记得曾经在花书上看到过，dropout可以认为是一种正则化的方式。但至此为止，我仅仅只知道它的作用和工作方式，具体原理仍是不知。

似乎在以往的经历中，这都是一个不起眼的参数，很少关注它到底设置成多少最合适。但似乎它并没有那么简单。

# 二，相关的先验知识

## 1，过拟合

在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。

过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。

# 三，作用与工作方式

这点很基础，简单的概括就是，随机按一定比例丢弃神经元，来防止过拟合。更加的具体的来说可能是随机让一定比例的神经元失活，让更多的神经元参与学习。让网络中的部分参数得到学习，即部分参数得到更新。可以消除减弱了神经元节点间的联合适应性，增强了泛化能力。

![img](https://github.com/xiejingcheng/xiejingcheng.github.io/raw/main/_posts\img\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xhZ29vbl9sYWxh,size_16,color_FFFFFF,t_70.png)

<center>图片来源：https://blog.csdn.net/lagoon_lala/article/details/118037530</center>

如上图所示，使用dropout方法训练, 每次update参数之前，对神经元(包括输入层不包括输出层)做抽样 ，每个神经元都有一定（即参数dropout rate）的几率被丢掉，跟它相连的权重也都要被丢掉。

但是囫囵吞枣的，我并不知道在降低过拟合外是否有其他影响，就比如在kaggle_GISLR_2023的训练中观测到的，高dropout rate使得训练集上的收敛速度下降，当然这很好理解随机失活了一部分神经元，其他神经元重新学习，使得学习速度下降，但似乎没有理解到更加深层次的影响。

# 四，dropout的一些理解

在我查询资料的过程中，常常看到两个说法，一个是ensemble，另一个是正则化。后面我们继续详细说这两种理解。

## 1，dropout与ensemble

这个似乎很好理解，通过随机失活一定比率的神经元，在每次训练时候使用不同的神经元，相当于训练了多个不同的模型，最终将这些模型的结果融合在一起。类似经常说的ensemble中的bagging方法。但是这是一种更加廉价的方法，一般的集成只能集成五到六个神经网络，但是dropout可以集成指数级数量的神经网络。

那么这里可以引出dropout处理过拟合的第一个原因：**取平均的作用**

先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

当然相比于其他集成，如bagging，dropout有一个特点：共享权重。bagging中的每一个模型是完全独立的，分别在训练集上训练到收敛。但是dropout中，每个子学习器都是集成自一个父类网络，

## 2，dropout与正则化

因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。

同时，由于dropout是一种正则化技术，它减少了模型的有效容量，为了抵消这种影响，我们必须增大模型。虽然在一般情况下，使用dropout会使得最佳验证集误差变小，但是这是以更大的模型和更多的训练周期为代价的。对于较大的数据集，dropout带来的收益可能会下降，甚至低于其所要付出的代价。

# 五，dropout rate设置方法

dropout的使用的一些技巧：

在最简单的情况下，每个单元都以固定的概率p保持独立于其他单元的概率，其中 p pp 可以使用验证集进行选择，也可以简单地设置为 0.5，对于大多数网络和网络来说，这似乎都接近于最佳值。但对于输入层，保留的最佳概率通常接近于1，而不是接近0.5。

对于LSTM，最好对输入和循环连接使用不同的丢失率。

隐藏层中的有效的Dropout Rate在0.5到0.8之间。输入层使用较大的 Dropout Rate，例如0.8。

一条好的经验法则是：将丢弃之前的层中的节点数除以建议的丢弃率，并将其用作使用丢弃的新网络中的节点数。例如，具有100个节点的网络在使用0.5的Dropout时，网络需要200个节点（100 / 0.5）

在数据集较少时使用:与其它正则化方法一样，对于训练数据量有限且模型可能过拟合训练数据而言，设置Dropout更为有效。训练数据足够时，使用Dropout可能会适得其反。对于非常大的数据集，正则化几乎没有减少泛化误差。在这些情况下，使用Dropout和较大模型的计算成本可能会超过进行正则化的好处。（wsy：所以使用dropout会变慢？）

# 参考

[理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)

[李宏毅ML笔记10: DL小妙招](https://blog.csdn.net/lagoon_lala/article/details/118037530)

[Dropout 不仅可以防止过拟合，也可以减小欠拟合？](https://zhuanlan.zhihu.com/p/611340706?utm_id=0)

[防止过拟合有哪些方法](https://zhuanlan.zhihu.com/p/32503623)

[深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)
