---
layout: post
title: dropout
tags: DL基础
math: true
date: 2023-3-26 14:56 +0800
---

# 一，前言

dropout rate是我之前经常遇到的一个参数，我之前并未觉得这个参数有多重要，总是随便写个0.3就完事了。但是这次的训练中出现了严重的过拟合现象，这时候我就想起了dropout rate这个参数，通过调整dropout rate似乎对过拟合有出奇好的效果。我记得曾经在花书上看到过，dropout可以认为是一种正则化的方式。但至此为止，我仅仅只知道它的作用和工作方式，具体原理仍是不知。

似乎在以往的经历中，这都是一个不起眼的参数，很少关注它到底设置成多少最合适。但似乎它并没有那么简单。

# 二，相关的先验知识



# 三，作用与工作方式

这点很基础，简单的概括就是，随机按一定比例丢弃神经元，来防止过拟合。更加的具体的来说可能是随机让一定比例的神经元失活，让更多的神经元参与学习。让网络中的部分参数得到学习，即部分参数得到更新。可以消除减弱了神经元节点间的联合适应性，增强了泛化能力。

![img](.\img\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xhZ29vbl9sYWxh,size_16,color_FFFFFF,t_70.png)

<center>图片来源：https://blog.csdn.net/lagoon_lala/article/details/118037530</center>

如上图所示，使用dropout方法训练, 每次update参数之前，对神经元(包括输入层不包括输出层)做抽样 ，每个神经元都有一定（即参数dropout rate）的几率被丢掉，跟它相连的权重也都要被丢掉。

但是囫囵吞枣的，我并不知道在降低过拟合外是否有其他影响，就比如在kaggle_GISLR_2023的训练中观测到的，高dropout rate使得训练集上的收敛速度下降，当然这很好理解随机失活了一部分神经元，其他神经元重新学习，使得学习速度下降，但似乎没有理解到更加深层次的影响。

# 四，dropout的一些理解

在我查询资料的过程中，常常看到两个说法，一个是ensemble，另一个是正则化。后面我们继续详细说这两种理解。

## 1，dropout与ensemble

这个似乎很好理解，通过随机失活一定比率的神经元，在每次训练时候使用不同的神经元，相当于训练了多个不同的模型，最终将这些模型的结果融合在一起。类似经常说的ensemble中的bagging方法。

## 2，dropout与正则化



# 五，dropout rate设置方法

# 六，dropout的改进

## 1，Stochastic depth

# 七，dropout与欠拟合



# 八，参考

[理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)

[李宏毅ML笔记10: DL小妙招](https://blog.csdn.net/lagoon_lala/article/details/118037530)

[Dropout 不仅可以防止过拟合，也可以减小欠拟合？](https://zhuanlan.zhihu.com/p/611340706?utm_id=0)

[防止过拟合有哪些方法](https://zhuanlan.zhihu.com/p/32503623)

